{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweet Retrieval Import\n",
    "\n",
    "import GetOldTweets3 as got\n",
    "import sys\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "#Sentiment Analysis Imports\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweetFetcher:\n",
    "\n",
    "### FUNCTIONS THAT HANDLE ACTOR LIST ###    \n",
    "    \n",
    "    #TODO needs to be changed to update actor_list based on how jennifer creates actor_lists in the other notebook\n",
    "\n",
    "    def create_movie_actor_list_from_file(self, input_actor_file):\n",
    "        \n",
    "        with open(input_actor_file, 'r') as input_file:\n",
    "            \n",
    "            data = input_file.read()\n",
    "\n",
    "            # parse file            \n",
    "            self.movie_actor_list = json.loads(data)\n",
    "        \n",
    "    def set_movie_actor_list(self, provided_actor_list):\n",
    "        self.movie_actor_list = provided_actor_list    \n",
    "        \n",
    "    def get_movie_actor_list(self):\n",
    "        return self.movie_actor_list\n",
    "    \n",
    "### FUNCTIONS THAT QUERY DATA\n",
    "    \n",
    "    # Query all movies using internal movie_actor_list \n",
    "    # Store results of all tweets\n",
    "    # Convert to JSON\n",
    "    # Store in Output file\n",
    "    \n",
    "    def query_tweets_as_JSON(self, output_file, tweets_per_actor, movie_limit):\n",
    "                    \n",
    "        with open(output_file, 'w') as write_file:\n",
    "\n",
    "            movie_tweet_object_list = []\n",
    "            processed_query_results = []\n",
    "            \n",
    "            movie_counter = 0\n",
    "            for movie in self.movie_actor_list:\n",
    "                if movie_counter > movie_limit:\n",
    "                    break            \n",
    "\n",
    "                title = movie['title'] \n",
    "                release_date = movie['release_date']\n",
    "                end_date = movie['end_date']\n",
    "                actors = movie['actors']\n",
    "                \n",
    "                if parse(release_date) < parse('2007-01-01'):\n",
    "                    continue\n",
    "                \n",
    "                movie_dictionary = {'title': title, 'actors': []}\n",
    "\n",
    "                for actor in actors:\n",
    "                    \n",
    "                    #Retrieve tweet objects for an actor and store them in a list\n",
    "                    actor_tweet_object_list = self.query_tweets(actor, release_date, end_date, tweets_per_actor)\n",
    "                    \n",
    "                    actor_dictionary = {'name': actor, 'tweets': []}\n",
    "                    \n",
    "                    movie_dictionary['actors'].append(actor_dictionary)\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    #Convert each tweet_object into its data\n",
    "                    for actor_tweet_object in actor_tweet_object_list:\n",
    "                        actor_dictionary['tweets'].append(self.parse_tweet_object([actor_tweet_object]))\n",
    "\n",
    "                \n",
    "\n",
    "                processed_query_results.append(movie_dictionary)\n",
    "                movie_counter += 1\n",
    "                \n",
    "                                        \n",
    "                                        \n",
    "            json.dump(processed_query_results, write_file)\n",
    "    \n",
    "    \n",
    "    # Use the GOT3 module to get a tweet\n",
    "    #Provide dates in \"YYYY-MM-DD\" Format\n",
    "    def query_tweets(self, query, start_date = \"2006-03-21\", end_date = \"2019-06-30\", max_tweets = 1):\n",
    "        tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query)\\\n",
    "                                               .setSince(start_date)\\\n",
    "                                               .setUntil(end_date)\\\n",
    "                                               .setMaxTweets(max_tweets)\n",
    "        \n",
    "        queried_tweet_object_list = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "        return queried_tweet_object_list\n",
    "\n",
    "    \n",
    "    \n",
    "    # Given a list of tweet objects, return a list of dictionaries\n",
    "    # where each dictionary looks like {date: strTweetDate, text: strText}\n",
    "    def parse_tweet_object(self, tweet_object_list):\n",
    "        \n",
    "        tweet_data_list = []\n",
    "            \n",
    "        for tweet in tweet_object_list:\n",
    "                            \n",
    "            date = str(tweet.date)\n",
    "            text = tweet.text\n",
    "            tweet_data = {\"date\": date, \"text\": text}\n",
    "\n",
    "            tweet_data_list.append(tweet_data)\n",
    "\n",
    "                                \n",
    "        return tweet_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Using TweetFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of how to use TweetFetcher\n",
    "\n",
    "def fetch_tweets(input_actor_file, output_tweet_file, tweets_per_actor, number_of_movies):\n",
    "    tweet_fetcher = tweetFetcher()    \n",
    "    \n",
    "    tweet_fetcher.create_movie_actor_list_from_file(input_actor_file)\n",
    "    \n",
    "    tweet_fetcher.query_tweets_as_JSON(output_tweet_file, tweets_per_actor, number_of_movies)\n",
    "\n",
    "    \n",
    "fetch_tweets(input_actor_file = \"intermediates/actors.json\",\n",
    "             output_tweet_file = \"intermediates/data_file.json\",\n",
    "             tweets_per_actor = 10,\n",
    "             number_of_movies = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentiment_analyzer:\n",
    "    \n",
    "    \n",
    "    #Given a selection of raw text, return the sentiment of that text\n",
    "    def get_text_sentiment(self, raw_text):\n",
    "        sentiment = TextBlob(raw_text).sentiment\n",
    "        return sentiment.polarity\n",
    "    \n",
    "    \n",
    "    # if verbose, will print \n",
    "    def mass_tweet_analysis(self, archive_file, write_file, verbose = False, logging_mode = False, actor_log_location = None, movie_log_location = None):\n",
    "        \n",
    "        with open(archive_file) as json_file:  \n",
    "            parsed = json.load(json_file)\n",
    "            \n",
    "            # Just in case - for ease of regression\n",
    "            if logging_mode:\n",
    "                \n",
    "                movie_sentiment_log = []\n",
    "                actor_sentiment_log = []\n",
    "            \n",
    "            movie_count = 0 \n",
    "            total_tweet_count = 0\n",
    "            total_actor_count = 0\n",
    "            \n",
    "            for movie in parsed:\n",
    "                \n",
    "                movie_count +=1 \n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Parsing: \" + movie['title'])\n",
    "            \n",
    "                movie_sentiment_sum = 0 \n",
    "                actor_count = 0 \n",
    "                \n",
    "                for actor in movie['actors']:\n",
    "                    \n",
    "                    actor_sentiment_sum = 0 \n",
    "                    tweet_count = 0 \n",
    "                    \n",
    "                    if len(actor['tweets']) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    for tweet in actor['tweets']:\n",
    "                        tweet = tweet[0]\n",
    "                        tweet['tweet_sentiment'] = self.get_text_sentiment(tweet['text'])\n",
    "                        \n",
    "                        tweet_count+=1\n",
    "                        total_tweet_count+=1\n",
    "                        actor_sentiment_sum += tweet['tweet_sentiment']\n",
    "                        \n",
    "                    actor_sentiment = actor_sentiment_sum / tweet_count\n",
    "                    actor['actor_sentiment'] = actor_sentiment\n",
    "                    \n",
    "                    if verbose:\n",
    "                        print(\"\\t Parsed \" + actor['name'])\n",
    "                    \n",
    "                    if logging_mode:    \n",
    "                        \n",
    "                        actor_sentiment_log.append((movie['title'], actor['name'], actor_sentiment))\n",
    "                    \n",
    "                    actor_count += 1\n",
    "                    total_actor_count +=1\n",
    "                    movie_sentiment_sum += actor_sentiment\n",
    "                    \n",
    "                movie_sentiment = movie_sentiment_sum / actor_count\n",
    "                movie['movie_sentiment'] = movie_sentiment\n",
    "                \n",
    "                if verbose:\n",
    "                \n",
    "                    print(\"Finished Parsing \" + movie['title'])\n",
    "                    print()\n",
    "                    \n",
    "                if movie_count % 100 == 0:\n",
    "                    print(\"Parsed \" + str(movie_count) + \" movies\")\n",
    "                \n",
    "                if logging_mode:\n",
    "                    \n",
    "                    movie_sentiment_log.append((movie['title'], movie_sentiment))\n",
    "            \n",
    "            if verbose:\n",
    "                \n",
    "                print(\"Parsed \"+str(movie_count)+\" movies\")\n",
    "                print(\"Parsed \"+str(total_actor_count)+ \" actors\")\n",
    "                print(\"Parsed \"+str(total_tweet_count)+ \" tweets\")\n",
    "                print()\n",
    "            \n",
    "            if logging_mode:\n",
    "                                                \n",
    "                with open(actor_log_location, 'w') as log_file:\n",
    "                    for item in actor_sentiment_log:\n",
    "                        log_file.write(\"\".join('%s %s %s \\n' % item))\n",
    "                \n",
    "                if verbose:\n",
    "                    \n",
    "                    print(\"Wrote actor sentiment logs to \" + actor_log_location)\n",
    "                    print()\n",
    "                        \n",
    "                with open(movie_log_location, 'w') as log_file:\n",
    "                    for item in movie_sentiment_log:\n",
    "                        log_file.write(\"\".join('%s %s \\n' % item))\n",
    "                    \n",
    "                if verbose:\n",
    "                    \n",
    "                    print(\"Wrote movie sentiment logs to \" + movie_log_location)\n",
    "                    print()\n",
    "                          \n",
    "            with open(write_file, 'w') as json_file: \n",
    "                    \n",
    "                json.dump(parsed, json_file)\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"Wrote JSON with sentiment to \"+write_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Using Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = sentiment_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Log, Silent Mode\n",
    "\n",
    "Computes sentiment from tweets in JSON archive_file and writes a new json blob to write_file.\n",
    "\n",
    "Only one alert is provided: every 100 movies parsed, it will alert on # of movies processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.mass_tweet_analysis(archive_file = \"intermediates/data_file.json\", \n",
    "                                       write_file = \"intermediates/sentiment_file.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Logging Mode and Silent Mode\n",
    "\n",
    "Computes sentiment from tweets in JSON archive_file and writes a new json blob to write_file.\n",
    "\n",
    "Only one alert is provided: every 100 movies parsed, it will alert on # of movies processed.\n",
    "\n",
    "Also produces two log files. \n",
    "\n",
    "actor_log_location parameter will be a file where each line is a tuple: (movie_title, actor_name, actor_sentiment)\n",
    "\n",
    "movie_log_location parameter will be a file where each line is a tuple: (movie_title, movie_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.mass_tweet_analysis(archive_file = \"intermediates/data_file.json\", \n",
    "                                       write_file = \"intermediates/sentiment_file.json\",\n",
    "                                       logging_mode = True,\n",
    "                                       actor_log_location = \"intermediates/actor_log\",\n",
    "                                       movie_log_location = \"intermediates/movie_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Log, Verbose Mode\n",
    "\n",
    "Computes sentiment from tweets in JSON archive_file and writes a new json blob to write_file.\n",
    "\n",
    "Along the way, provides updates on completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.mass_tweet_analysis(archive_file = \"intermediates/data_file.json\", \n",
    "                                       write_file = \"intermediates/sentiment_file.json\",\n",
    "                                       verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Logging Mode, Verbose Mode\n",
    "\n",
    "Computes sentiment from tweets in JSON archive_file and writes a new json blob to write_file.\n",
    "\n",
    "Along the way, provides updates, and produces log files as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer.mass_tweet_analysis(archive_file = \"intermediates/data_file.json\", \n",
    "                                       write_file = \"intermediates/sentiment_file.json\",\n",
    "                                       verbose = True,\n",
    "                                       logging_mode = True,\n",
    "                                       actor_log_location = \"intermediates/actor_log\",\n",
    "                                       movie_log_location = \"intermediates/movie_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
